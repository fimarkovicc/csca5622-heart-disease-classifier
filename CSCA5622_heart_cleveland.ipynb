{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cb9ae11",
   "metadata": {},
   "source": [
    "# CSCA 5622 — Supervised Learning Final Project (Heart Disease — Cleveland)\n",
    "\n",
    "**Task:** Binary classification — predict presence of heart disease from clinical attributes (Cleveland subset of UCI Heart Disease).  \n",
    "**Dataset license:** CC BY 4.0 (UCI ML Repository).  \n",
    "**Run date:** 2025-09-29 09:45 UTC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2c064a",
   "metadata": {},
   "source": [
    "## 1) Data Provenance & Context\n",
    "\n",
    "- **Source:** UCI Machine Learning Repository — *Heart Disease* (Cleveland subset). `id=45` in UCI.  \n",
    "  - DOI/Citation: Janosi, Steinbrunn, Pfisterer, & Detrano (1989). Heart Disease [Dataset]. UCI ML Repository.  \n",
    "- **License:** Creative Commons Attribution 4.0 (CC BY 4.0).  \n",
    "- **Attributes used (14):** `age, sex, cp, trestbps, chol, fbs, restecg, thalach, exang, oldpeak, slope, ca, thal, num(target)`  \n",
    "  - Target is `num` (0..4). We binarize to **presence = 1** if `num > 0`, else **0**.\n",
    "- **Known quirks:** Missing values in **ca** and **thal** (often encoded as `?`). We'll clean and impute appropriately.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a154b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_validate, GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    ConfusionMatrixDisplay,\n",
    "    RocCurveDisplay,\n",
    "    roc_auc_score\n",
    ")\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "pd.set_option('display.max_columns', 100)\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92bd49b",
   "metadata": {},
   "source": [
    "## 2) Load Data\n",
    "\n",
    "We first try the **official UCI helper `ucimlrepo`** to fetch the dataset.  \n",
    "On success, we **cache** a CSV locally (same folder) for reproducibility.\n",
    "\n",
    "> If you're offline, re-run later with internet, or manually place `heart_cleveland_uci.csv` beside this notebook (columns: the 14 Cleveland features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dea4a025",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Fetch via ucimlrepo (preferred). If unavailable/offline, show an instructive error.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_uci_cleveland\u001b[39m() -> \u001b[43mpd\u001b[49m.DataFrame:\n\u001b[32m      4\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m      5\u001b[39m         \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mucimlrepo\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m fetch_ucirepo\n",
      "\u001b[31mNameError\u001b[39m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# Fetch via ucimlrepo (preferred). If unavailable/offline, show an instructive error.\n",
    "\n",
    "def load_uci_cleveland() -> pd.DataFrame:\n",
    "    try:\n",
    "        from ucimlrepo import fetch_ucirepo\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\n",
    "            \"`ucimlrepo` not installed. Run: `pip install ucimlrepo` and re-run.\"\n",
    "        )\n",
    "    heart = fetch_ucirepo(id=45)  # Heart Disease\n",
    "    # UCI helper returns features/targets as DataFrames\n",
    "    X = heart.data.features.copy()\n",
    "    y = heart.data.targets.copy()\n",
    "    # In the UCI helper, Cleveland subset is typically what the docs demonstrate; however,\n",
    "    # some mirrors provide combined or per-site files. We'll align to the 14-feature schema.\n",
    "    # Compose a single DF if needed.\n",
    "    if 'num' not in y.columns:\n",
    "        # Sometimes target column is named differently\n",
    "        # Attempt common fallbacks\n",
    "        for cand in ['target','disease','diagnosis','class']:\n",
    "            if cand in y.columns:\n",
    "                y = y.rename(columns={cand: 'num'})\n",
    "                break\n",
    "        if 'num' not in y.columns:\n",
    "            y = y.rename(columns={y.columns[0]: 'num'})\n",
    "    df = pd.concat([X.reset_index(drop=True), y.reset_index(drop=True)], axis=1)\n",
    "    return df\n",
    "\n",
    "CACHE_PATH = Path('heart_cleveland_uci.csv')\n",
    "\n",
    "try:\n",
    "    if CACHE_PATH.exists():\n",
    "        df = pd.read_csv(CACHE_PATH)\n",
    "    else:\n",
    "        df = load_uci_cleveland()\n",
    "        # Keep only the canonical 14 columns if present\n",
    "        canonical_cols = ['age','sex','cp','trestbps','chol','fbs','restecg','thalach','exang','oldpeak','slope','ca','thal','num']\n",
    "        # If all canonical present, filter and save\n",
    "        if set(canonical_cols).issubset(df.columns):\n",
    "            df = df[canonical_cols]\n",
    "        df.to_csv(CACHE_PATH, index=False)\n",
    "    print('Loaded data with shape:', df.shape)\n",
    "    display(df.head())\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\n",
    "        f\"Could not load UCI Cleveland dataset automatically.\\n\"\n",
    "        f\"Reason: {e}\\n\\n\"\n",
    "        \"Fix: Ensure internet is available, then:\\n\"\n",
    "        \"  pip install ucimlrepo\\n\"\n",
    "        \"  # and re-run this cell.\\n\\n\"\n",
    "        \"Alternatively, download from UCI and save as 'heart_cleveland_uci.csv' beside this notebook.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23cbb05",
   "metadata": {},
   "source": [
    "## 3) Cleaning & Target Definition\n",
    "\n",
    "- Convert `?` to `NaN` (notably **ca**, **thal**).  \n",
    "- Coerce numeric types.  \n",
    "- Create binary target `target = 1 if num > 0 else 0`.  \n",
    "- Drop rows with missing target, then impute numeric/categorical as needed (inside pipelines to avoid leakage)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a2ce8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace '?' with NaN and coerce numerics\n",
    "na_before = df.isna().sum()\n",
    "df = df.replace('?', np.nan)\n",
    "\n",
    "# Coerce specific columns to numeric (errors='coerce' turns bad parses into NaN)\n",
    "for col in ['age','sex','cp','trestbps','chol','fbs','restecg','thalach','exang','oldpeak','slope','ca','thal','num']:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "print('Missing values per column (after coercion):')\n",
    "print(df.isna().sum().sort_values(ascending=False).head(10))\n",
    "\n",
    "# Binarize target: presence(1) if num > 0 else 0\n",
    "if 'target' not in df.columns:\n",
    "    df['target'] = (df['num'] > 0).astype(int)\n",
    "\n",
    "# Basic sanity checks\n",
    "print('\\nClass balance (0=no disease, 1=disease):')\n",
    "print(df['target'].value_counts(), (df['target'].value_counts()/len(df)).round(3))\n",
    "\n",
    "# We'll keep rows where essential features are present; remaining NaNs will be imputed in pipelines\n",
    "essential = ['age','sex','cp','trestbps','chol','fbs','restecg','thalach','exang','oldpeak','slope','ca','thal','target']\n",
    "df = df.dropna(subset=['target'])\n",
    "print('Remaining rows after ensuring target present:', len(df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb6a1df",
   "metadata": {},
   "source": [
    "## 4) EDA Highlights\n",
    "\n",
    "- Univariate distributions  \n",
    "- Correlation with target  \n",
    "- Quick boxplots for most informative features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e997e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Univariate\n",
    "subset_cols = [c for c in ['age','trestbps','chol','thalach','oldpeak','ca'] if c in df.columns]\n",
    "_ = df[subset_cols].hist(figsize=(10,8))\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# Correlation with target (numeric-only)\n",
    "corr = df.corr(numeric_only=True)['target'].sort_values(ascending=False)\n",
    "print('Top correlations with target:')\n",
    "print(corr.head(10))\n",
    "print('\\nLeast correlations with target:')\n",
    "print(corr.tail(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8decad0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplots of top 6 absolute correlations (excluding 'target' itself)\n",
    "abs_corr = df.corr(numeric_only=True)['target'].abs().sort_values(ascending=False)\n",
    "feat_order = [c for c in abs_corr.index if c != 'target'][:6]\n",
    "import math\n",
    "rows = math.ceil(len(feat_order)/3)\n",
    "fig, axes = plt.subplots(rows, 3, figsize=(12, 4*rows))\n",
    "axes = axes.flatten()\n",
    "for i, f in enumerate(feat_order):\n",
    "    axes[i].boxplot([df[df['target']==0][f].dropna(), df[df['target']==1][f].dropna()], labels=['no disease','disease'])\n",
    "    axes[i].set_title(f)\n",
    "for j in range(i+1, len(axes)):\n",
    "    axes[j].axis('off')\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063561d3",
   "metadata": {},
   "source": [
    "## 5) Train/Test Split & Preprocessing\n",
    "\n",
    "- Split with stratification.  \n",
    "- Numeric features: Standardize.  \n",
    "- Categorical features (if any remain as discrete codes): One-hot encode.  \n",
    "- Impute missing values **within** the pipeline to avoid leakage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27211d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "feature_cols = ['age','sex','cp','trestbps','chol','fbs','restecg','thalach','exang','oldpeak','slope','ca','thal']\n",
    "feature_cols = [c for c in feature_cols if c in df.columns]\n",
    "X = df[feature_cols].copy()\n",
    "y = df['target'].copy()\n",
    "\n",
    "# Identify numeric vs categorical by heuristic: integer-coded categories\n",
    "categorical_like = [c for c in ['sex','cp','fbs','restecg','exang','slope','ca','thal'] if c in X.columns]\n",
    "numeric_like = [c for c in X.columns if c not in categorical_like]\n",
    "\n",
    "numeric_tf = Pipeline([\n",
    "    ('impute', SimpleImputer(strategy='median')),\n",
    "    ('scale', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_tf = Pipeline([\n",
    "    ('impute', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocess = ColumnTransformer([\n",
    "    ('num', numeric_tf, numeric_like),\n",
    "    ('cat', categorical_tf, categorical_like)\n",
    "])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "print('Train/Test sizes:', X_train.shape, X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1956cd",
   "metadata": {},
   "source": [
    "## 6) Baseline Model Comparison (CV)\n",
    "\n",
    "We compare: Logistic Regression, SVC (RBF), Random Forest, Gradient Boosting.  \n",
    "Scoring: **accuracy, F1, ROC AUC**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aeb1c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'LogisticRegression': Pipeline([\n",
    "        ('prep', preprocess),\n",
    "        ('clf', LogisticRegression(max_iter=1000))\n",
    "    ]),\n",
    "    'SVC (RBF)': Pipeline([\n",
    "        ('prep', preprocess),\n",
    "        ('clf', SVC(probability=True))\n",
    "    ]),\n",
    "    'RandomForest': Pipeline([\n",
    "        ('prep', preprocess),\n",
    "        ('clf', RandomForestClassifier(random_state=42))\n",
    "    ]),\n",
    "    'GradientBoosting': Pipeline([\n",
    "        ('prep', preprocess),\n",
    "        ('clf', GradientBoostingClassifier(random_state=42))\n",
    "    ])\n",
    "}\n",
    "\n",
    "scoring = {'accuracy':'accuracy', 'f1':'f1', 'roc_auc':'roc_auc'}\n",
    "cv_results = {}\n",
    "for name, pipe in models.items():\n",
    "    scores = cross_validate(pipe, X_train, y_train, scoring=scoring, cv=cv, n_jobs=-1)\n",
    "    cv_results[name] = {k.replace('test_',''): float(np.mean(v)) for k,v in scores.items() if k.startswith('test_')}\n",
    "\n",
    "pd.DataFrame(cv_results).T.sort_values('roc_auc', ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edfb431",
   "metadata": {},
   "source": [
    "## 7) Hyperparameter Tuning — choose a top model (SVC)\n",
    "\n",
    "We tune `C` and `gamma` for an RBF SVC using 5-fold CV and ROC AUC as the refit metric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9851dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_pipe = models['SVC (RBF)']\n",
    "param_grid = {\n",
    "    'clf__C': [0.1, 1, 10, 30],\n",
    "    'clf__gamma': ['scale', 0.1, 0.01, 0.001]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(svc_pipe, param_grid, scoring='roc_auc', cv=cv, n_jobs=-1)\n",
    "gs.fit(X_train, y_train)\n",
    "print('Best params:', gs.best_params_)\n",
    "print('Best CV ROC AUC:', round(gs.best_score_, 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da41e1c",
   "metadata": {},
   "source": [
    "## 8) Final Evaluation on Holdout Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb566846",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = gs.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_proba = best_model.predict_proba(X_test)[:,1]\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=['no disease','disease']))\n",
    "print('Test ROC AUC:', round(roc_auc_score(y_test, y_proba), 4))\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred)\n",
    "plt.title('Confusion Matrix — Best SVC')\n",
    "plt.show()\n",
    "\n",
    "RocCurveDisplay.from_predictions(y_test, y_proba)\n",
    "plt.title('ROC Curve — Best SVC')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e24bfc8",
   "metadata": {},
   "source": [
    "## 9) Interpretability Notes\n",
    "\n",
    "- For linear models (e.g., **Logistic Regression**), inspect standardized coefficients to see directional effects.\n",
    "- For tree ensembles, use **permutation importance** and **partial dependence**; for SVC, prefer **model-agnostic** methods (Permutation Importance, SHAP).\n",
    "- Beware of **category encodings** impacting interpretation (one-hot reference levels).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8431db8d",
   "metadata": {},
   "source": [
    "## 10) Conclusions & Next Steps\n",
    "\n",
    "**Summary**\n",
    "- Cleaned Cleveland dataset (handled `?` missing values) and binarized `num`.\n",
    "- Compared 4 models; tuned SVC for ROC AUC.\n",
    "- Reported holdout metrics & diagnostic plots.\n",
    "\n",
    "**Limitations**\n",
    "- Small tabular clinical dataset; potential site-specific biases.\n",
    "- Binary reduction of multi-class severity (`num`) discards granularity.\n",
    "\n",
    "**Next Steps**\n",
    "- Explore threshold tuning for cost-sensitive decisions (FN vs FP).\n",
    "- Calibrate probabilities (Platt/Isotonic) and evaluate PR curves (class imbalance).\n",
    "- Add SHAP for feature attribution; test alternative learners (XGBoost/LightGBM).\n",
    "\n",
    "**Reproducibility**\n",
    "- Cache created: `heart_cleveland_uci.csv`. Commit code, not large/raw external data. Provide data download instructions in README.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
